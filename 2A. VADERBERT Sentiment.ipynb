{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import transformers\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.25.1\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.BertConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "_start_time = time.time()\n",
    "\n",
    "def tic():\n",
    "    global _start_time \n",
    "    _start_time = time.time()\n",
    "\n",
    "def tac():\n",
    "    t_sec = round(time.time() - _start_time)\n",
    "    (t_min, t_sec) = divmod(t_sec,60)\n",
    "    (t_hour,t_min) = divmod(t_min,60) \n",
    "    print('Time passed: {}hour:{}min:{}sec'.format(t_hour,t_min,t_sec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text(text):\n",
    "    '''Make text lowercase, remove text in square brackets, remove punctuation, remove words containing numbers, and remove dots (except .com).'''\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[\\[\\]]\", \" \", text)\n",
    "    text = re.sub('[%s]' % re.escape('!\"#&()*+,-/:;<=>?@^_`{|}~'), ' ', text)\n",
    "    text = re.sub('\\n', ' ', text)\n",
    "    text = re.sub(r'(?<!\\w)(\\.)((?!\\bcom\\b)\\w)', r' \\2', text)  # Remove dots except for .com\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "cleaner = lambda x: clean_text(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Check if CUDA is available and set the device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize outside the function to avoid loading every time\n",
    "model_name = 'bert-base-uncased'\n",
    "model = BertForSequenceClassification.from_pretrained(model_name).to(device)\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "def financial_insights(transcript):\n",
    "    # Set the maximum sequence length\n",
    "    max_seq_length = 512\n",
    "    \n",
    "    # Tokenize the transcript\n",
    "    tokenized_text = tokenizer.tokenize(transcript)\n",
    "\n",
    "    # Account for [CLS] and [SEP] tokens\n",
    "    max_tokens = max_seq_length - 2\n",
    "    if len(tokenized_text) > max_tokens:\n",
    "        tokenized_text = tokenized_text[:max_tokens]\n",
    "\n",
    "    # Add special tokens and convert to input IDs\n",
    "    input_ids = [tokenizer.cls_token_id] + tokenizer.convert_tokens_to_ids(tokenized_text) + [tokenizer.sep_token_id]\n",
    "    input_ids = torch.tensor(input_ids).unsqueeze(0).to(device)\n",
    "\n",
    "    # Get the model's predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "    predicted_labels = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "    # Extract the relevant information from the transcript\n",
    "    entities = {\n",
    "        'positive_indicators': ['growth', 'revenue', 'profit'],\n",
    "        'negative_indicators': ['decline', 'loss', 'risk'],\n",
    "    }\n",
    "\n",
    "    # Find sentences containing the keywords\n",
    "    sentences = transcript.split('. ')\n",
    "    result = {}\n",
    "\n",
    "    for entity, keywords in entities.items():\n",
    "        result[entity] = []\n",
    "        for sentence in sentences:\n",
    "            if any(keyword in sentence.lower() for keyword in keywords):\n",
    "                result[entity].append(sentence)\n",
    "    return result\n",
    "\n",
    "def sentiment_analysis(data):\n",
    "    sentiment_score = []\n",
    "    for i in range(len(data)):\n",
    "        sentence = \"\".join(data[i])\n",
    "        sentiment = sid.polarity_scores(sentence)\n",
    "        sentiment_score.append(sentiment)\n",
    "    return sentiment_score\n",
    "\n",
    "\n",
    "def dicts_operation(positive_indicators_dict, negative_indicators_dict):\n",
    "    combined_dict, result_dict = [], {}\n",
    "    for i in range(len(positive_indicators_dict)):\n",
    "        for key in positive_indicators_dict[i]:\n",
    "            result_dict[key] = (positive_indicators_dict[i][key] + negative_indicators_dict[i][key])/2\n",
    "        combined_dict.append(result_dict)\n",
    "        result_dict = {}\n",
    "    return combined_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9869 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully Processed VADERBERT S&P2006\n",
      "Time passed: 0hour:4min:1sec\n",
      "Successfully Processed VADERBERT S&P2007\n",
      "Time passed: 0hour:9min:35sec\n",
      "Successfully Processed VADERBERT S&P2008\n",
      "Time passed: 0hour:24min:16sec\n",
      "Successfully Processed VADERBERT S&P2009\n",
      "Time passed: 0hour:24min:45sec\n",
      "Successfully Processed VADERBERT S&P2010\n",
      "Time passed: 0hour:27min:49sec\n",
      "Successfully Processed VADERBERT S&P2011\n",
      "Time passed: 0hour:30min:33sec\n",
      "Successfully Processed VADERBERT S&P2012\n",
      "Time passed: 0hour:31min:11sec\n",
      "Successfully Processed VADERBERT S&P2013\n",
      "Time passed: 0hour:31min:38sec\n",
      "Successfully Processed VADERBERT S&P2014\n",
      "Time passed: 0hour:32min:0sec\n",
      "Successfully Processed VADERBERT S&P2015\n",
      "Time passed: 0hour:32min:13sec\n",
      "Successfully Processed VADERBERT S&P2016\n",
      "Time passed: 0hour:32min:54sec\n",
      "Successfully Processed VADERBERT S&P2017\n",
      "Time passed: 0hour:33min:42sec\n",
      "Successfully Processed VADERBERT S&P2018\n",
      "Time passed: 0hour:34min:29sec\n",
      "Successfully Processed VADERBERT S&P2019\n",
      "Time passed: 0hour:34min:43sec\n",
      "Successfully Processed VADERBERT S&P2020\n",
      "Time passed: 0hour:35min:2sec\n",
      "Successfully Processed VADERBERT S&P2021\n",
      "Time passed: 0hour:31min:52sec\n",
      "Successfully Processed VADERBERT S&P2022\n",
      "Time passed: 0hour:32min:11sec\n",
      "Successfully Processed VADERBERT S&P2023\n",
      "Time passed: 0hour:12min:15sec\n"
     ]
    }
   ],
   "source": [
    "for i in range(2006, 2024):\n",
    "    tic()\n",
    "    data = pd.read_pickle(f\"Clean_S&P{i}.pkl\")\n",
    "    transcripts = data[\"content\"].apply(cleaner) # List of transcripts\n",
    "    results = [financial_insights(transcript) for transcript in transcripts]\n",
    "\n",
    "    # Extract positive_indicators and negative_indicators from results\n",
    "    positive_indicators = [results[i][\"positive_indicators\"] for i in range(len(results))]\n",
    "    negative_indicators = [results[i][\"negative_indicators\"] for i in range(len(results))]\n",
    "    data[\"positive_indicators\"], data[\"negative_indicators\"] = positive_indicators, negative_indicators\n",
    "    data[\"sentiment\"] = dicts_operation(sentiment_analysis(data['positive_indicators']), sentiment_analysis(data['negative_indicators']))\n",
    "    diff_value = []\n",
    "    for u in range(len(data[\"sentiment\"])): \n",
    "        diff = data[\"sentiment\"][u][\"pos\"] - data[\"sentiment\"][u][\"neg\"] \n",
    "        diff_value.append(diff)\n",
    "    data[\"pos_neg_diff\"] = pd.Series(diff_value)\n",
    "    data.to_pickle(f'VADERBERT_S&P{i}.pkl')\n",
    "    print(f\"Successfully Processed VADERBERT S&P{i}\")\n",
    "    tac()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined pickle file has been saved as VADERBERT_S&P.pkl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def combine_pickles(start_year=2006, end_year=2023):\n",
    "    # Initialize an empty dataframe\n",
    "    combined_df = pd.DataFrame()\n",
    "\n",
    "    # Loop over the range of years\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        pickle_file = f\"VADERBERT_S&P{year}.pkl\"\n",
    "\n",
    "        # Check if the pickle file exists\n",
    "        if os.path.exists(pickle_file):\n",
    "            # Load the data from the pickle file\n",
    "            df = pd.read_pickle(pickle_file)\n",
    "            \n",
    "            # Append the data to the combined dataframe\n",
    "            combined_df = pd.concat([combined_df, df])\n",
    "        else:\n",
    "            print(f\"Pickle file for year {year} does not exist.\")\n",
    "    \n",
    "    # Save the combined dataframe to a new pickle file\n",
    "    combined_df.to_pickle(\"VADERBERT_S&P.pkl\")\n",
    "    print(\"Combined pickle file has been saved as VADERBERT_S&P.pkl\")\n",
    "\n",
    "# Call the function\n",
    "combine_pickles()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
